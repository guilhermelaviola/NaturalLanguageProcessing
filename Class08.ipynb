{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzqCdU42PMliFpbnKZwQVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/NaturalLanguageProcessing/blob/main/Class08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition**\n",
        "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing that aims to identify and classify entities such as people, places, dates, and organizations in unstructured text, transforming it into structured data for computational analysis. A key component of this process is manual entity annotation, which, despite being time-consuming, is essential for training accurate machine learning models. Tools such as NLTK and spaCy support NER by offering efficient text processing and pre-trained models for multiple languages. NER has wide-ranging applications, including question answering, information extraction, machine translation, and intelligent assistants, where it enhances accuracy, preserves meaning, and enables systems to understand and act on user input. Overall, NER plays a crucial role in advancing NLP by enabling deeper understanding and effective use of textual data."
      ],
      "metadata": {
        "id": "Q0Wt6JQ10V0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rta5VeDtBzS_",
        "outputId": "d765bb82-14f1-46b3-a06c-aeb7620408d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2026.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries and resources:\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "import wikipedia\n",
        "import os"
      ],
      "metadata": {
        "id": "FupHvkqUB0tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: Named Entity Recognition in Texts**\n",
        "Named Entity Recognition (NER) is an essential task in the field of Natural Language Processing (NLP) that consists of identifying and classifying significant semantic elements in texts, such as names of people, organizations, places, and dates. The importance of NER lies in its ability to assign meaning and structure to raw textual data, facilitating the understanding and automatic analysis of such data."
      ],
      "metadata": {
        "id": "L38IYpoaCAk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the English language template:\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Example text for entity recognition:\n",
        "text = 'Barack Obama was born in Honolulu, Hawaii, on August 4, 1961.'\n",
        "\n",
        "# Text processing:\n",
        "doc = nlp(text)\n",
        "\n",
        "# Displaying the recognized entities:\n",
        "for entity in doc.ents:\n",
        "  print(entity.text, entity.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joKsfLu_CEsB",
        "outputId": "bf23e206-ccfd-47f9-9f8e-fa14b168d439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack Obama PERSON\n",
            "Honolulu GPE\n",
            "Hawaii GPE\n",
            "August 4, 1961 DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: Named Entity Annotation**\n",
        "Named entity annotation is a critical process in Natural Language Processing (NLP), involving the identification and classification of text segments as meaningful entities, such as names of people, places, organizations, and others. This step is fundamental for training machine learning models capable of automatically processing and interpreting large volumes of text."
      ],
      "metadata": {
        "id": "a_0hmurkCeRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Cristiano Ronaldo played for Real Madrid.'\n",
        "rules = {'player': ['Cristiano Ronaldo'], 'club': ['Real Madrid']}\n",
        "\n",
        "def annotate_text(text, rules):\n",
        "  for entity, terms in rules.items():\n",
        "    for term in terms:\n",
        "      text = text.replace(term, f'<{entity}>{term}</{entity}>')\n",
        "      return text\n",
        "\n",
        "annotated_text = annotate_text(text, rules)\n",
        "print(annotated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MonY6P7WCmgE",
        "outputId": "7ecef360-b5f3-43ca-ac97-d0e566f45699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<player>Cristiano Ronaldo</player> played for Real Madrid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: NLTK and SpaCy Libraries for REN**\n",
        "In the context of Named Entity Recognition (REN) in Natural Language Processing (NLP), the NLTK (Natural Language Toolkit) and spaCy libraries are essential tools. Both offer pre-trained models that facilitate the identification and classification of entities in texts. However, it is crucial to understand that these generic models may not cover all the specificities of different domains, which sometimes requires manual annotation and training of specific models."
      ],
      "metadata": {
        "id": "_gWwEvI-DH-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Changed to download the specific English tagger resource\n",
        "nltk.download('maxent_ne_chunker_tab') # This line is added to download the missing data\n",
        "\n",
        "text = 'Henrikh Mkhitaryan was born in Yerevan.'\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tags = nltk.pos_tag(tokens)\n",
        "entities = nltk.chunk.ne_chunk(tags)\n",
        "\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MKdfKBJDPHW",
        "outputId": "fd26c379-7f36-4e3c-8afd-a96a4909d1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Henrikh/NNP)\n",
            "  (PERSON Mkhitaryan/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Yerevan/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}