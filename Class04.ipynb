{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObLSQaq6px7Jc6PUwO8e6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/NaturalLanguageProcessing/blob/main/Class04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Token & Sentence Segmentation**\n",
        "Natural language processing (NLP) focuses on enabling computers to understand and analyze human language, with text preprocessing playing a vital role in preparing raw data for analysis. Key preprocessing techniques include sentence segmentation, which breaks text into meaningful sentences, and tokenization, which further divides sentences into smaller units such as words or symbols. Additional steps like stopword removal can improve efficiency depending on the task. Tools such as the NLTK library in Python provide flexible and effective methods for performing these processes across multiple languages. These techniques are especially important in tasks like sentiment analysis, where analyzing smaller text units allows for more accurate and detailed insights. Overall, selecting appropriate preprocessing methods is essential for building effective NLP systems and has wide-ranging applications in areas such as market research, customer service, and social media analytics."
      ],
      "metadata": {
        "id": "kEZH4PKFy5Tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries:\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "vWy7G7lCOAAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68d949a-9ccf-4027-b9c4-49391c5239a5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example: Sentence Segmentation with NLTK**\n",
        "Using the NLTK library for sentence segmentation, it is possible to automate and improve the process of separating text into sentences, essential for various NLP tasks. In this example, we work with the DHBB dataset, provided by the Getúlio Vargas Foundation, containing entries on Brazilian political figures."
      ],
      "metadata": {
        "id": "-Sxv-8yRzPS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "NLTK, or the Natural Language Toolkit, is an open-source Python library designed for working with\n",
        "human language data. It provides a comprehensive suite of tools for performing text analysis,\n",
        "including tokenization, stemming, tagging, parsing, and machine learning for linguistic tasks.\n",
        "'''\n",
        "\n",
        "# Assuming that text is the string containing the DHBB text:\n",
        "sentences = sent_tokenize(text, language='english')\n",
        "\n",
        "# Assuming that 'sentences' is the list containing the previously segmented sentences:\n",
        "tokenized_sentences = [word_tokenize(sentence, language='english') for sentence in sentences]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "tokens = word_tokenize(text, language='english')\n",
        "tokens_without_stopwords = [token for token in tokens if token.lower() not in stop_words and token not in punctuation]"
      ],
      "metadata": {
        "id": "SeEWX1BazPlr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKVCcCePN7ZM"
      },
      "outputs": [],
      "source": [
        "with open('dhbb.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming text into sentences manually:\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('  ', ' ')\n",
        "text = text.replace('', '')\n",
        "\n",
        "text\n",
        "\n",
        "sentences = text.split('.')\n",
        "\n",
        "sentences\n",
        "\n",
        "sentences = [sentence.strip() + '.' for sentence in sentences if sentence]\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "id": "OqOcHQCmOQ2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming text into sentences with NLTK:\n",
        "nltk.download('punkt')\n",
        "\n",
        "text\n",
        "\n",
        "nltk_sentences = nltk.tokenize.sent_tokenize(text, language='english')\n",
        "\n",
        "nltk_sentences"
      ],
      "metadata": {
        "id": "d3CEijw9PZj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences manually:\n",
        "tokenized_sentences = []\n",
        "\n",
        "for sentence in nltk_sentences:\n",
        "  sentence = sentence.replace('.', ' . ')\n",
        "  sentence = sentence.replace(',', ' , ')\n",
        "  sentence = sentence.replace(':', ' : ')\n",
        "  sentence = sentence.replace('(', ' ( ')\n",
        "  sentence = sentence.replace(')', ' ) ')\n",
        "  sentence = sentence.replace('-', ' - ')\n",
        "  sentence = sentence.replace('«', ' « ')\n",
        "  sentence = sentence.replace('»', ' » ')\n",
        "  sentence = sentence.replace('  ', ' ')\n",
        "  sentence = sentence.strip()\n",
        "  tokenized_sentences.append(sentence.split(' '))\n",
        "\n",
        "tokenized_sentences\n",
        "\n",
        "tokenized_sentences[8]"
      ],
      "metadata": {
        "id": "Ki86eJTePzLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences with NLTK:\n",
        "tokenized_sentences_nltk = []\n",
        "\n",
        "for sentence in nltk_sentences:\n",
        "    sentence = nltk.tokenize.word_tokenize(sentence)\n",
        "    tokenized_sentences_nltk.append(sentence)\n",
        "\n",
        "tokenized_sentences_nltk\n",
        "\n",
        "tokenized_sentences_nltk[8]"
      ],
      "metadata": {
        "id": "2HFAAaSaQfEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords manually:\n",
        "! wget -O stopwords.txt https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\n",
        "\n",
        "with open('stopwords.txt') as f:\n",
        "    stopwords = f.read()\n",
        "\n",
        "print(stopwords)\n",
        "\n",
        "stopwords = stopwords.split('\\n')\n",
        "\n",
        "stopwords\n",
        "\n",
        "tokenized_sentences_nltk\n",
        "\n",
        "sentences_without_stopwords = []\n",
        "\n",
        "for sentence in tokenized_sentences_nltk:\n",
        "  sentences_without_stopwords = []\n",
        "  for token in sentence:\n",
        "    if not token.lower() in stopwords:\n",
        "      sentences_without_stopwords.append(token)\n",
        "      sentences_without_stopwords.append(sentences_without_stopwords)\n",
        "\n",
        "sentences_without_stopwords"
      ],
      "metadata": {
        "id": "yi1u1d8YXlSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords with NLTK:\n",
        "string.punctuation\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.corpus.stopwords.words('english')\n",
        "\n",
        "sentences_without_stopwords_nltk = []\n",
        "\n",
        "for sentence in sentences_without_stopwords_nltk:\n",
        "  sentences_without_stopwords = []\n",
        "  for token in sentence:\n",
        "    if not token.lower() in nltk.corpus.stopwords.words('english') and not token in string.punctuation:\n",
        "      sentences_without_stopwords.append(token)\n",
        "      sentences_without_stopwords_nltk.append(sentences_without_stopwords)\n",
        "\n",
        "sentences_without_stopwords_nltk"
      ],
      "metadata": {
        "id": "lM38RuXKYZ1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}