{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXpGXwp8ddFqT99XDyxyn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/NaturalLanguageProcessing/blob/main/Class04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries:\n",
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "vWy7G7lCOAAU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKVCcCePN7ZM"
      },
      "outputs": [],
      "source": [
        "with open('dhbb.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming text into sentences manually:\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('  ', ' ')\n",
        "text = text.replace('', '')\n",
        "\n",
        "text\n",
        "\n",
        "sentences = text.split('.')\n",
        "\n",
        "sentences\n",
        "\n",
        "sentences = [sentence.strip() + '.' for sentence in sentences if sentence]\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "id": "OqOcHQCmOQ2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming text into sentences with NLTK:\n",
        "nltk.download('punkt')\n",
        "\n",
        "text\n",
        "\n",
        "nltk_sentences = nltk.tokenize.sent_tokenize(text, language='english')\n",
        "\n",
        "nltk_sentences"
      ],
      "metadata": {
        "id": "d3CEijw9PZj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences manually:\n",
        "tokenized_sentences = []\n",
        "\n",
        "for sentence in nltk_sentences:\n",
        "  sentence = sentence.replace('.', ' . ')\n",
        "  sentence = sentence.replace(',', ' , ')\n",
        "  sentence = sentence.replace(':', ' : ')\n",
        "  sentence = sentence.replace('(', ' ( ')\n",
        "  sentence = sentence.replace(')', ' ) ')\n",
        "  sentence = sentence.replace('-', ' - ')\n",
        "  sentence = sentence.replace('«', ' « ')\n",
        "  sentence = sentence.replace('»', ' » ')\n",
        "  sentence = sentence.replace('  ', ' ')\n",
        "  sentence = sentence.strip()\n",
        "  tokenized_sentences.append(sentence.split(' '))\n",
        "\n",
        "tokenized_sentences\n",
        "\n",
        "tokenized_sentences[8]"
      ],
      "metadata": {
        "id": "Ki86eJTePzLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences with NLTK:\n",
        "tokenized_sentences_nltk = []\n",
        "\n",
        "for sentence in nltk_sentences:\n",
        "    sentence = nltk.tokenize.word_tokenize(sentence)\n",
        "    tokenized_sentences_nltk.append(sentence)\n",
        "\n",
        "tokenized_sentences_nltk\n",
        "\n",
        "tokenized_sentences_nltk[8]"
      ],
      "metadata": {
        "id": "2HFAAaSaQfEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords manually:\n",
        "! wget -O stopwords.txt https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt\n",
        "\n",
        "with open('stopwords.txt') as f:\n",
        "    stopwords = f.read()\n",
        "\n",
        "print(stopwords)\n",
        "\n",
        "stopwords = stopwords.split('\\n')\n",
        "\n",
        "stopwords\n",
        "\n",
        "tokenized_sentences_nltk\n",
        "\n",
        "sentences_without_stopwords = []\n",
        "\n",
        "for sentence in tokenized_sentences_nltk:\n",
        "  sentences_without_stopwords = []\n",
        "  for token in sentence:\n",
        "    if not token.lower() in stopwords:\n",
        "      sentences_without_stopwords.append(token)\n",
        "      sentences_without_stopwords.append(sentences_without_stopwords)\n",
        "\n",
        "sentences_without_stopwords"
      ],
      "metadata": {
        "id": "yi1u1d8YXlSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords with NLTK:\n",
        "string.punctuation\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.corpus.stopwords.words('english')\n",
        "\n",
        "sentences_without_stopwords_nltk = []\n",
        "\n",
        "for sentence in sentences_without_stopwords_nltk:\n",
        "  sentences_without_stopwords = []\n",
        "  for token in sentence:\n",
        "    if not token.lower() in nltk.corpus.stopwords.words('english') and not token in string.punctuation:\n",
        "      sentences_without_stopwords.append(token)\n",
        "      sentences_without_stopwords_nltk.append(sentences_without_stopwords)\n",
        "\n",
        "sentences_without_stopwords_nltk"
      ],
      "metadata": {
        "id": "lM38RuXKYZ1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}